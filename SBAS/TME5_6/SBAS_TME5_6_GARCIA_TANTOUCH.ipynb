{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzMJ2QtGxpkq"
   },
   "source": [
    "<h1><b>Statistique en Bioinformatique : </b> TME 5 et 6 </h1>\n",
    "<br>\n",
    "L’objectif de ce TME est:\n",
    "<br>\n",
    "<ul>\n",
    "<li> implémenter l'algorithme de Viterbi et l'estimation des paramètres (en utilisant le Viterbi training)\n",
    "pour l'exemple du occasionally dishonest casino.   </li> \n",
    "</ul>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p><b>Soumission</b></p>\n",
    "<ul>\n",
    "<li>Renomer le fichier TME5_6.ipynb pour TME_5_6_NomEtudiant1_NomEtudiant2.ipynb </li>\n",
    "<li>Soumettre via moodle </li>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiohDpWCxpkv"
   },
   "source": [
    "Nom etudiant 1 : GARCIA Enzo\n",
    "<br>\n",
    "Nom etudiant 2 : TANTOUCH Maher\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6n5Srpuxpkv"
   },
   "source": [
    "### Introduction\n",
    "Un casino parfois malhonnête (*occasionally dishonest casino*) utilise 2 types de pièces : **fair** et **unfair**.\n",
    "\n",
    "Hidden States: S = \\{F,U\\} (Fair, Unfair)\n",
    "\n",
    "La matrice de transition entre les états cachés est :\n",
    "\n",
    "P =\n",
    "\\begin{pmatrix}\n",
    "0.99 & 0.01 \\\\\n",
    "0.05 & 0.95\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "<br> Et la condition initiale $\\pi^{(0)} = (0.999,0.001)$ \n",
    "Le jeux commence presque toujours avec le pieces juste (fair).\n",
    "\n",
    "\n",
    "Obserations/Symbols: O = {H,T} (head, tail):\n",
    "\n",
    "Les probabilités d'émission des symboles:\n",
    "\n",
    "\\begin{aligned}\n",
    "e_F(H) &= 0.5 \\quad & e_F(T) &= 0.5 \\\\\n",
    "e_U(H) &= 0.9 \\quad & e_U(T) &= 0.1\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKz1fEeIxpkw"
   },
   "source": [
    "<b>Exercice 1</b>:\n",
    "<u>Simulation</u>: Écrire une fonction qui simule $T$ jets de pièces. \n",
    "La fonction renverra un tableau à deux colonnes correspondant \n",
    "aux valeurs simulées pour les états cachés $X_t$ \n",
    "(type de dés utilisée, “F” ou “U”) et aux symboles observées $Y_t$ \n",
    "(résultat du jet de dés, “H” ou “T”). On simulera une séquence\n",
    "de longueur 2000 qu'on gardera pour les applications ultérieures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "txZ2YCPbxpkx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#states\n",
    "S = { 0:'F',1 :'U'}\n",
    "\n",
    "#transition probability matrix\n",
    "Pij = np.array([[0.99,0.01], [0.05,0.95]])\n",
    "\n",
    "#emision symbols \n",
    "O = {0:'H', 1: 'T'}\n",
    "\n",
    "#emission probability matrix\n",
    "Ei = np.array([[0.5,0.5], [0.9,0.1]]) #ça aurait dû être Eio\n",
    "\n",
    "# initial Condition\n",
    "pi0=np.array([0.999,0.001])\n",
    "\n",
    "#number of jets\n",
    "T = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U T\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "U H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F T\n",
      "F H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "U H\n",
      "U H\n",
      "F H\n",
      "F H\n",
      "F T\n",
      "F H\n",
      "F T\n",
      "F H\n"
     ]
    }
   ],
   "source": [
    "def jets(T, pi0, Ei, Pij):\n",
    "    \"\"\"\n",
    "    Simulation du Occasionally Dishonest Casino\n",
    "    input1 T: nombre de jets\n",
    "    input2 pi0: condition initiale\n",
    "    input3 Ei: matrice des probabilités d'émission\n",
    "    input4 Pij: matrice des probabilités de transition\n",
    "    output: matrice Tx2 contenant la séquence des états cachés et des observations\n",
    "    \"\"\"\n",
    "    \n",
    "    #on initialise et choix au hasard d'un etat initial\n",
    "    jetsRes = np.zeros((T, 2), dtype=int)\n",
    "    etat = np.random.choice([0, 1], p=pi0)\n",
    "    \n",
    "    for t in range(T):\n",
    "        #stockage de l'etat caché\n",
    "        jetsRes[t, 0] = etat\n",
    "        \n",
    "        #on genere l'observation a partir de l'etat czché et transition vers nouveau etat\n",
    "        observation = np.random.choice([0, 1], p=Ei[etat])\n",
    "        jetsRes[t, 1] = observation\n",
    "\n",
    "        etat = np.random.choice([0, 1], p=Pij[etat])\n",
    "    \n",
    "    return jetsRes\n",
    "\n",
    "def printSimulation(simulation):\n",
    "    for t in simulation:\n",
    "        print(S[t[0]], O[t[1]])\n",
    "\n",
    "#pour generer la simulation\n",
    "jetsRes = jets(T, pi0, Ei, Pij)\n",
    "printSimulation(jetsRes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpGzwVdIxpk0"
   },
   "source": [
    "<b>Exercice 2</b>: <u>Algorithme de Viterbi </u>: Écrire une fonction qui permet\n",
    "de déterminer la séquence $(i^\\star_t)_{t=0:T}$ d'états cachés\n",
    "plus probable, ainsi que sa probabilité. En tant qu'observations, utiliser le résultat de la \n",
    "simulation (2éme colonne) de la question 1. Comparer $(i^\\star_t)_{t=0:T}$ avec\n",
    "les vrais états cachés (1ère colonne de la simulation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur d'estimation de Viterbi:\n",
      "6.9 %\n",
      "Probabilité estimée:\n",
      "0.0\n",
      "Erreur d'estimation de Viterbi:\n",
      "6.0 %\n",
      "Probabilité estimée:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def viterbi(simulations, Pij, Ei, pi0, enLog):\n",
    "    \"\"\"\n",
    "    Implémente l'algorithme de Viterbi\n",
    "    input1 simulations: matrice |TxN_states| contenant les observations\n",
    "    input2 Pij: matrice de transition\n",
    "    input3 Ei: matrice d'émission\n",
    "    input4 pi0: condition initiale\n",
    "    input5 enLog: bool, applique log si True pour éviter l'underflow\n",
    "    output1 i_star: séquence d'états la plus probable\n",
    "    output2 prob: probabilité associée à cette séquence\n",
    "    \"\"\"\n",
    "    obs = simulations[:, 1]\n",
    "    nS = len(Pij)  #nbr d'etats\n",
    "    T = len(obs)  #nbr d'obs\n",
    "    \n",
    "    if enLog:\n",
    "        Pij = np.log(Pij)\n",
    "        Ei = np.log(Ei)\n",
    "        pi0 = np.log(pi0)\n",
    "    \n",
    "    delta = np.zeros((T, nS))\n",
    "    psi = np.zeros((T, nS), dtype=int)\n",
    "    \n",
    "    #initialisation\n",
    "    delta[0] = pi0 + Ei[:, obs[0]] if enLog else pi0 * Ei[:, obs[0]]\n",
    "    \n",
    "    #recursion\n",
    "    for t in range(1, T):\n",
    "        for j in range(nS):\n",
    "            trans_probs = delta[t - 1] + Pij[:, j] if enLog else delta[t - 1] * Pij[:, j]\n",
    "            psi[t, j] = np.argmax(trans_probs)\n",
    "            delta[t, j] = trans_probs[psi[t, j]] + Ei[j, obs[t]] if enLog else trans_probs[psi[t, j]] * Ei[j, obs[t]]\n",
    "    \n",
    "    #terminaison\n",
    "    i_star = np.zeros(T, dtype=int)\n",
    "    i_star[-1] = np.argmax(delta[-1])\n",
    "    prob = np.max(delta[-1]) if not enLog else np.exp(np.max(delta[-1]))\n",
    "    \n",
    "    #retropropagation\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        i_star[t] = psi[t + 1, i_star[t + 1]]\n",
    "    \n",
    "    return i_star, prob\n",
    "\n",
    "def analyseResultats(simulations, estimation):\n",
    "    \"\"\"\n",
    "    Compare le chemin estimé et le chemin réel\n",
    "    input1 simulations: chemin réel\n",
    "    input2 estimation: chemin estimé\n",
    "    output1 error: pourcentage d'erreur\n",
    "    \"\"\"\n",
    "    etats_reels = simulations[:, 0]\n",
    "    erreurs = np.sum(etats_reels != estimation)\n",
    "    error = (erreurs / len(etats_reels)) * 100\n",
    "    return error\n",
    "\n",
    "#execution et affichage des resultats\n",
    "i_est, p_est = viterbi(jetsRes, Pij, Ei, pi0, False)\n",
    "error = analyseResultats(jetsRes, i_est)\n",
    "print(\"Erreur d'estimation de Viterbi:\")\n",
    "print(error, \"%\")\n",
    "print(\"Probabilité estimée:\")\n",
    "print(p_est)\n",
    "\n",
    "i_est, p_est = viterbi(jetsRes, Pij, Ei, pi0, True)\n",
    "error = analyseResultats(jetsRes, i_est)\n",
    "print(\"Erreur d'estimation de Viterbi:\")\n",
    "print(error, \"%\")\n",
    "print(\"Probabilité estimée:\")\n",
    "print(p_est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7NimDYwxpk2"
   },
   "source": [
    "<b>Exercice 3</b>: <u>Estimation des paramètres</u>\n",
    "<br>\n",
    "3.1) Écrire une fonction qu'utilise tous les résultats de la simulation\n",
    "(états et symboles) pour estimer $P_{ij}$, $E_i(O)$ et pi_0. Attention, pour éviter les probabilités à zéro nous allons utiliser les pseudo-count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pij estimé:\n",
      "[[0.98899826 0.01100174]\n",
      " [0.06884058 0.93115942]]\n",
      "\n",
      "Eio estimé:\n",
      "[[0.48553241 0.51446759]\n",
      " [0.91666667 0.08333333]]\n",
      "\n",
      "pi0 estimé:\n",
      "[0.999 0.001]\n"
     ]
    }
   ],
   "source": [
    "def nombresOccurrence(simulation, nS, nO):\n",
    "    \"\"\"\n",
    "    Estimation des paramètres par comptage\n",
    "    input1 simulation: matrice |TxN_states| contenant les données\n",
    "    input2 nS: nombre d'états\n",
    "    input3 nO: nombre d'observations\n",
    "    output1 Pij_est: matrice des probabilités de transition\n",
    "    output2 Ei_est: matrice des probabilités d'émission\n",
    "    output3 pi0_est: condition initiale\n",
    "    \"\"\"\n",
    "    \n",
    "    Pij_est = np.ones((nS, nS))  ##pseudo-count\n",
    "    Ei_est = np.ones((nS, nO))  \n",
    "    pi0_est = np.ones((nS)) / 1000  # Valeur initiale faible\n",
    "    \n",
    "    etat_seq = simulation[:, 0]\n",
    "    obs_seq = simulation[:, 1]\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    #pour le cpmptage des transitions et émissions\n",
    "    for t in range(T - 1):\n",
    "        Pij_est[etat_seq[t], etat_seq[t + 1]] += 1\n",
    "        Ei_est[etat_seq[t], obs_seq[t]] += 1\n",
    "    Ei_est[etat_seq[-1], obs_seq[-1]] += 1  #derniere obs\n",
    "    \n",
    "    #on met a jour la vzleur de pi0_est\n",
    "    pi0_est[etat_seq[0]] = 0.999\n",
    "    \n",
    "    #on normalise\n",
    "    Pij_est /= Pij_est.sum(axis=1, keepdims=True)\n",
    "    Ei_est /= Ei_est.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return Pij_est, Ei_est, pi0_est\n",
    "\n",
    "Pij_est, Ei_est, pi0_est = nombresOccurrence(jetsRes, 2, 2)\n",
    "\n",
    "print('Pij estimé:')\n",
    "print(Pij_est)\n",
    "print('\\nEio estimé:')\n",
    "print(Ei_est)\n",
    "print('\\npi0 estimé:')\n",
    "print(pi0_est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-ZYvpCgxpk3"
   },
   "source": [
    "3.2) <u> Viterbi training </u>: Écrire une fonction qui utilise\n",
    "seulement la séquence $(O_t)_{t=0:T}$ (2emme colonne de la simulation) pour estimer les\n",
    "paramètres $P_{ij}$ est $Ei(O)$. On s’arrêtera quand les différences entre les logVraissamblance est inférieur à 1e-04. Comparer les résultats de 3.1 et de 3.2 (3.2 avec plusieurs restarts,\n",
    "et avec initialisation des paramètres aléatoire).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle est convergé après 2 itérations.\n",
      "\n",
      "Pij estimée:\n",
      "[[0.98956522 0.01043478]\n",
      " [0.06569343 0.93430657]]\n",
      "\n",
      "Eij estimée:\n",
      "[[0.48521739 0.51478261]\n",
      " [0.91970803 0.08029197]]\n",
      "\n",
      "pi0 estimée:\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pour eviter de diviser par zero, on definit un epsilon\n",
    "epsilon = 1e-10\n",
    "\n",
    "#on initialise de facon aleatoire Pij, Eij, pi0\n",
    "def InititRandom(nS, nO):\n",
    "    \"\"\"\n",
    "    Initialisation aléatoire des matrices Pij, Ei et pi0\n",
    "    input1 nS: nombre d'états\n",
    "    input2 nO: nombre d'observations\n",
    "    output1 Pij_init: matrice des probabilités de transition\n",
    "    output2 Ei_init: matrice des probabilités d'émission\n",
    "    output3 pi0_init: condition initiale\n",
    "    \"\"\"\n",
    "    random.seed(10)\n",
    "    Pij_init = np.random.rand(nS, nS) + epsilon  # Ajouter un epsilon pour éviter zéro\n",
    "    Ei_init = np.random.rand(nS, nO) + epsilon  # Ajouter un epsilon pour éviter zéro\n",
    "    pi0_init = np.random.rand(nS) + epsilon  # Ajouter un epsilon pour éviter zéro\n",
    "    \n",
    "    # Normalisation pour que les sommes de chaque ligne soient égales à 1\n",
    "    Pij_init /= Pij_init.sum(axis=1, keepdims=True)\n",
    "    Ei_init /= Ei_init.sum(axis=1, keepdims=True)\n",
    "    pi0_init /= pi0_init.sum()\n",
    "    \n",
    "    return Pij_init, Ei_init, pi0_init\n",
    "\n",
    "#pour le log vraisemblance\n",
    "def logLikelihood(Pij_est, Eij_est, pi0_est, jets_est):\n",
    "    \"\"\"\n",
    "    Calcule la log-vraisemblance des paramètres donnés\n",
    "    input1 Pij: matrice de transition\n",
    "    input2 Ei: matrice d'émission\n",
    "    input3 pi0: condition initiale\n",
    "    input4 jets: matrice |Tx2| contenant les données\n",
    "    output: log-vraisemblance\n",
    "    \"\"\"\n",
    "    etat_seq = jets_est[:, 0]\n",
    "    obs_seq = jets_est[:, 1]\n",
    "    T = len(obs_seq)  #nbr d'obs\n",
    "    lLikelihood = np.log(pi0_est[etat_seq[0]] + epsilon) + np.log(Eij_est[etat_seq[0], obs_seq[0]] + epsilon)\n",
    "\n",
    "    for t in range(1, T):\n",
    "        lLikelihood += np.log(Pij_est[etat_seq[t-1], etat_seq[t]] + epsilon) + np.log(Eij_est[etat_seq[t], obs_seq[t]] + epsilon)\n",
    "\n",
    "    return lLikelihood\n",
    "\n",
    "def Training(jets, nS, nO):\n",
    "    \"\"\"\n",
    "    Viterbi Training\n",
    "    input1 jets: matrice |Tx2| contenant les données\n",
    "    input2 nS: nombre d'états\n",
    "    input3 nO: nombre d'observations\n",
    "    output1 Pij_est: matrice des probabilités de transition\n",
    "    output2 Ei_est: matrice des probabilités d'émission\n",
    "    output3 pi0_est: condition initiale\n",
    "    output4 lLikelihood: log vraisemblance à chaque itération\n",
    "    \"\"\"\n",
    "    jets_est = np.array(jets)    \n",
    "    Pij_est, Ei_est, pi0_est = InititRandom(nS, nO)\n",
    "    \n",
    "    maxIteration = 10000\n",
    "    iCount = 0\n",
    "    stopping_criterion = 1e-04  # Le critère d'arrêt pour la différence de log-vraisemblance\n",
    "    lLikelihood = np.zeros(maxIteration)\n",
    "    prev_likelihood = -np.inf  # Initialisation d'une log-vraisemblance précédente faible\n",
    "   \n",
    "    while iCount < maxIteration:\n",
    "        #on calcul la log-vraisemblance actuelle\n",
    "        lLikelihood[iCount] = logLikelihood(Pij_est, Ei_est, pi0_est, jets_est)\n",
    "        \n",
    "        #on verifie la convergence\n",
    "        if np.abs(lLikelihood[iCount] - prev_likelihood) < stopping_criterion:\n",
    "            break\n",
    "        prev_likelihood = lLikelihood[iCount]\n",
    "        \n",
    "        # E-step (Calcul des probabilités de Viterbi pour les états cachés)\n",
    "        delta = np.zeros((len(jets_est), nS))  #proba des etats a chaque temps\n",
    "        psi = np.zeros((len(jets_est), nS), dtype=int)  #on suit les transitions max\n",
    "        for t in range(len(jets_est)):\n",
    "            for s in range(nS):\n",
    "                #probabilite pour chaque etat\n",
    "                delta[t, s] = np.log(pi0_est[s] + epsilon) + np.log(Ei_est[s, jets_est[t, 1]] + epsilon)\n",
    "                \n",
    "        # (mise à jour des parametres Pij, Ei, pi0)\n",
    "        Pij_est = np.zeros((nS, nS))\n",
    "        Ei_est = np.zeros((nS, nO))\n",
    "        pi0_est = np.zeros(nS)\n",
    "        \n",
    "        #on met a jour les transitions et les emissions\n",
    "        for t in range(len(jets_est)-1):\n",
    "            i = jets_est[t, 0]  # État à l'instant t\n",
    "            j = jets_est[t+1, 0]  # État à l'instant t+1\n",
    "            Pij_est[i, j] += 1\n",
    "            Ei_est[i, jets_est[t, 1]] += 1\n",
    "\n",
    "        #on met a jour la condition initiale pi0\n",
    "        pi0_est[jets_est[0, 0]] += 1\n",
    "        \n",
    "        #on normalise\n",
    "        Pij_est /= Pij_est.sum(axis=1, keepdims=True)\n",
    "        Ei_est /= Ei_est.sum(axis=1, keepdims=True)\n",
    "        pi0_est /= pi0_est.sum()\n",
    "\n",
    "        iCount += 1\n",
    "    \n",
    "    return Pij_est, Ei_est, pi0_est, lLikelihood[:iCount]\n",
    "\n",
    "#on execute la fonction Viterbi Training et affiche les resultats\n",
    "Pij_est, Ei_est, pi0_est, lLikelihood = Training(jetsRes, 2, 2)\n",
    "\n",
    "itCount = len(lLikelihood)\n",
    "print('Le modèle est convergé après ' + str(itCount) + ' itérations.')\n",
    "print('\\nPij estimée:')\n",
    "print(Pij_est)\n",
    "print('\\nEij estimée:')\n",
    "print(Ei_est)\n",
    "print('\\npi0 estimée:')\n",
    "print(pi0_est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6B-AX8Zxpk4"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Remark: La méthode 3.2 semble donner une estimation des paramètres plus précise, avec une convergence plus rapide et des probabilités mieux définies pour les transitions et l'état initial.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7iq2fv5xpk5"
   },
   "source": [
    "3.3) <u>Viterbi training deuxième version</u>. \n",
    "<BR>Écrivez une version de 3.2 qui:\n",
    "- part plusieurs fois (100x) d'une initialisation aléatoire desparamètres de l'HMM,\n",
    "- utilise Viterbi training pour estimer les paramètres,\n",
    "- calcul la log-vraisemblance pour les paramètres estimés,\n",
    "- sauvegarde seulement l'estimation avec la valeur maximale de lalog-vraisemblance.\n",
    "\n",
    "Qu'est-ce que vous observez?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maher\\AppData\\Local\\Temp\\ipykernel_54348\\435899875.py:72: RuntimeWarning: divide by zero encountered in log\n",
      "  delta[t, s] = np.log(pi0_est[s]) + np.log(Ei_est[s, jets_est[t, 1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur Pij estimée:\n",
      "[[0.98956522 0.01043478]\n",
      " [0.06569343 0.93430657]]\n",
      "\n",
      "Meilleur Eij estimée:\n",
      "[[0.48521739 0.51478261]\n",
      " [0.91970803 0.08029197]]\n",
      "\n",
      "Meilleur pi0 estimée:\n",
      "[1. 0.]\n",
      "\n",
      "Meilleure log-vraisemblance:\n",
      "-1438.6637915308331\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialisation aléatoire des matrices Pij, Ei, pi0\n",
    "def InititRandom(nS, nO):\n",
    "    random.seed(10)\n",
    "    Pij_init = np.random.rand(nS, nS)\n",
    "    Ei_init = np.random.rand(nS, nO)\n",
    "    pi0_init = np.random.rand(nS)\n",
    "    \n",
    "    # Normalisation pour que les sommes de chaque ligne soient égales à 1\n",
    "    Pij_init /= Pij_init.sum(axis=1, keepdims=True)\n",
    "    Ei_init /= Ei_init.sum(axis=1, keepdims=True)\n",
    "    pi0_init /= pi0_init.sum()\n",
    "    \n",
    "    return Pij_init, Ei_init, pi0_init\n",
    "\n",
    "# Calcule log vraisemblance\n",
    "def logLikelihood(Pij_est, Eij_est, pi0_est, jets_est):\n",
    "    etat_seq = jets_est[:, 0]\n",
    "    obs_seq = jets_est[:, 1]\n",
    "    T = len(obs_seq)\n",
    "    lLikelihood = np.log(pi0_est[etat_seq[0]]) + np.log(Eij_est[etat_seq[0], obs_seq[0]])\n",
    "\n",
    "    for t in range(1, T):\n",
    "        lLikelihood += np.log(Pij_est[etat_seq[t-1], etat_seq[t]]) + np.log(Eij_est[etat_seq[t], obs_seq[t]])\n",
    "\n",
    "    return lLikelihood\n",
    "\n",
    "# Fonction pour le Viterbi Training version 2.0\n",
    "def TrainingV2(jets, nS, nO, nIterat=100):\n",
    "    \"\"\"\n",
    "    Viterbi Training version 2.0\n",
    "    input1 jets: matrice |Tx2| contenant les données\n",
    "    input2 nS: nombre d'états\n",
    "    input3 nO: nombre d'observations\n",
    "    input4 nIterat: nombre d'initialisations aléatoires\n",
    "    output1 Pij_best: meilleure matrice des probabilités de transition\n",
    "    output2 Ei_best: meilleure matrice des probabilités d'émission\n",
    "    output3 pi0_best: meilleure condition initiale \n",
    "    output4 lLikelihood_best: meilleure log-vraisemblance\n",
    "    \"\"\"\n",
    "    Pij_best = None\n",
    "    Ei_best = None\n",
    "    pi0_best = None\n",
    "    lLikelihood_best = -np.inf  #on initialise avec une valeur faible\n",
    "\n",
    "    #on realise plusieurs initialisations aléatoires\n",
    "    for i in range(nIterat):\n",
    "        Pij_est, Ei_est, pi0_est = InititRandom(nS, nO)  #pour l'initialisation aleatoire\n",
    "        jets_est = np.array(jets)\n",
    "        \n",
    "        maxIteration = 10000\n",
    "        iCount = 0\n",
    "        stopping_criterion = 1e-04\n",
    "        prev_likelihood = -np.inf\n",
    "        lLikelihood = np.zeros(maxIteration)\n",
    "        \n",
    "        while iCount < maxIteration:\n",
    "            lLikelihood[iCount] = logLikelihood(Pij_est, Ei_est, pi0_est, jets_est)\n",
    "            \n",
    "            #on verifie ici aussi la convergence\n",
    "            if np.abs(lLikelihood[iCount] - prev_likelihood) < stopping_criterion:\n",
    "                break\n",
    "            prev_likelihood = lLikelihood[iCount]\n",
    "\n",
    "            #calcul des probab de Viterbi pour les etats caches\n",
    "            delta = np.zeros((len(jets_est), nS))\n",
    "            psi = np.zeros((len(jets_est), nS), dtype=int)\n",
    "            for t in range(len(jets_est)):\n",
    "                for s in range(nS):\n",
    "                    delta[t, s] = np.log(pi0_est[s]) + np.log(Ei_est[s, jets_est[t, 1]])\n",
    "\n",
    "            #mise a jour des parametres Pij, Ei, pi0\n",
    "            Pij_est = np.zeros((nS, nS))\n",
    "            Ei_est = np.zeros((nS, nO))\n",
    "            pi0_est = np.zeros(nS)\n",
    "            \n",
    "            for t in range(len(jets_est)-1):\n",
    "                i = jets_est[t, 0]\n",
    "                j = jets_est[t+1, 0]\n",
    "                Pij_est[i, j] += 1\n",
    "                Ei_est[i, jets_est[t, 1]] += 1\n",
    "\n",
    "            pi0_est[jets_est[0, 0]] += 1\n",
    "            \n",
    "            #on normalise\n",
    "            Pij_est /= Pij_est.sum(axis=1, keepdims=True)\n",
    "            Ei_est /= Ei_est.sum(axis=1, keepdims=True)\n",
    "            pi0_est /= pi0_est.sum()\n",
    "\n",
    "            iCount += 1\n",
    "        \n",
    "        #on sauvegarde les meilleurs parametres en fonction de la log-vraisemblance\n",
    "        if lLikelihood[iCount-1] > lLikelihood_best:\n",
    "            Pij_best = Pij_est\n",
    "            Ei_best = Ei_est\n",
    "            pi0_best = pi0_est\n",
    "            lLikelihood_best = lLikelihood[iCount-1]\n",
    "\n",
    "    return Pij_best, Ei_best, pi0_best, lLikelihood_best\n",
    "\n",
    "#on execute la fonction du Viterbi Training version 2.0 + affichage des resultats\n",
    "nIterat = 100\n",
    "Pij_best, Ei_best, pi0_best, lLikelihood_best = TrainingV2(jetsRes, 2, 2, nIterat)\n",
    "\n",
    "print('Meilleur Pij estimée:')\n",
    "print(Pij_best)\n",
    "print('\\nMeilleur Eij estimée:')\n",
    "print(Ei_best)\n",
    "print('\\nMeilleur pi0 estimée:')\n",
    "print(pi0_best)\n",
    "print('\\nMeilleure log-vraisemblance:')\n",
    "print(lLikelihood_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4EHk9MKxpk6"
   },
   "source": [
    "<font color=\"blue\">\n",
    "Remark: \n",
    "    - On remarque tout d'abord une convergence vers les mêmes valeurs (Pij, Eij et Pi(0) obtenues sont identiques à la première version de Viterbi malgré l'initialisation aléatoire faite dans la Viterbi 2. De même, les paramètres ne varient pas à travers les initialisations aléatoires ce qui suggère que le modèle est relativement robuste ici et que la convergence vers la solution optimale est fiable.\n",
    "    - Après 100 tentatives on a une Log-vraisemblance de -1440 ce qui indique que le modèle est bien ajusté aux données.\n",
    "    - Malgré les différentes initialisation aléatoires (ce qui permet de se défaire du biais de maximul local), le modèle converge toujours vers la même solution ce qui suggère un maximum global.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TME5_6_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
